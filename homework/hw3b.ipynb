{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HW3B - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "See Canvas for details on how to complete and submit this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This assignment transitions you from NumPy's numerical array operations to Pandas' powerful tabular data manipulation. While NumPy excels at homogeneous numerical arrays, Pandas is designed for the heterogeneous, labeled data that characterizes most real-world datasets—mixing dates, categories, numbers, and text within the same table.\n",
    "\n",
    "You'll work with real bike share data from Chicago's Divvy system to answer questions about urban transportation patterns. Through three progressively complex problems—exploring usage patterns, analyzing rider behavior, and conducting temporal analysis—you'll discover why Pandas has become the standard tool for data analysis in Python.\n",
    "\n",
    "The assignment emphasizes Pandas' design philosophy: named column access, explicit indexing methods (loc/iloc), handling missing data, and method chaining for readable data pipelines. You'll also see how Pandas builds on NumPy while adding the structure and convenience needed for practical data science work.\n",
    "\n",
    "This assignment should take 3-5 hours to complete.\n",
    "\n",
    "Before submitting, ensure your notebook:\n",
    "\n",
    "- Runs completely with \"Kernel → Restart & Run All\"\n",
    "- Includes thoughtful responses to all interpretation questions\n",
    "- Uses clear variable names and follows good coding practices\n",
    "- Shows your work (don't just print final answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "By completing this assignment, you will be able to:\n",
    "\n",
    "1. **Construct and manipulate Pandas data structures**\n",
    "   - Create DataFrames from dictionaries and CSV files\n",
    "   - Distinguish between Series and DataFrame objects\n",
    "   - Set and reset index structures appropriately\n",
    "   - Understand when operations return views vs copies\n",
    "2. **Apply explicit indexing paradigms**\n",
    "   - Use `loc[]` for label-based data access\n",
    "   - Use `iloc[]` for position-based data access\n",
    "   - Access columns using bracket notation\n",
    "   - Explain when each indexing method is appropriate\n",
    "3. **Diagnose and explore datasets systematically**\n",
    "   - Use `info()`, `describe()`, `head()`, and `dtypes` to understand data structure\n",
    "   - Identify missing values with `isna()` and `notna()`\n",
    "   - Calculate summary statistics across different axes\n",
    "   - Interpret value distributions with `value_counts()`\n",
    "4. **Filter data with boolean indexing and queries**\n",
    "   - Combine multiple conditions with `&`, `|`, and `~` operators\n",
    "   - Use `isin()` for membership testing\n",
    "   - Apply `query()` for readable complex filters\n",
    "   - Understand how index alignment affects operations\n",
    "5. **Work with datetime data**\n",
    "   - Parse dates during CSV loading\n",
    "   - Extract temporal components with the `.dt` accessor\n",
    "   - Filter data by date ranges\n",
    "   - Create time-based derived features\n",
    "6. **Connect Pandas patterns to data analysis workflows**\n",
    "   - Formulate questions that data can answer\n",
    "   - Choose appropriate methods for different analysis tasks\n",
    "   - Interpret results in domain context\n",
    "   - Recognize when vectorized operations outperform apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generative AI Allowance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "You may use GenAI tools for brainstorming, explanations, and code sketches if you disclose it, understand it, and validate it. Your submission must represent your own work and you are solely responsible for its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Total of 90 points available, will be graded out of 80. Scores of >100% are allowed.\n",
    "\n",
    "Distribution:\n",
    "\n",
    "- Tasks: 48 pts\n",
    "- Interpretation: 32 pts\n",
    "- Reflection: 10 pts\n",
    "\n",
    "Points by Problem:\n",
    "\n",
    "- Problem 1: 3 tasks, 10 pts\n",
    "- Problem 2: 4 tasks, 14 pts\n",
    "- Problem 3: 4 tasks, 14 pts\n",
    "- Problem 4: 3 tasks, 10 pts\n",
    "\n",
    "Interpretation Questions:\n",
    "\n",
    "- Problem 1: 3 questions, 8 pts\n",
    "- Problem 2: 4 questions, 8 pts\n",
    "- Problem 3: 3 questions, 8 pts\n",
    "- Problem 4: 3 questions, 8 pts\n",
    "\n",
    "Graduate differentiation: poor follow-up responses will result in up to a 5pt deduction for that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Dataset: Chicago Divvy Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The dataset you will analyze is based on real trip information from Divvy, Chicago's bike share system. It contains individual trips with start/end times, station information, and rider type.\n",
    "\n",
    "Dataset homepage: https://divvybikes.com/system-data\n",
    "\n",
    "Each trip includes:\n",
    "\n",
    "- Trip start and end times (datetime)\n",
    "- Start and end station names and IDs\n",
    "- Rider type (member vs casual)\n",
    "- Bike type (classic, electric, or docked)\n",
    "\n",
    "Chicago's Department of Transportation uses this data to optimize station placement, understand usage patterns, and improve service. You'll explore similar questions that real transportation analysts investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Problem 1: Creating DataFrames from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Before loading data from files, you need to understand how Pandas structures are built. In this problem, you'll create Series and DataFrames manually using Python's built-in data structures. This is a quick warmup to establish the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Task 1a: Create a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a Series called `temperatures` representing daily high temperatures for a week:\n",
    "\n",
    "- Monday: 72°F\n",
    "- Tuesday: 75°F  \n",
    "- Wednesday: 68°F\n",
    "- Thursday: 71°F\n",
    "- Friday: 73°F\n",
    "\n",
    "Use the day names as the index. Print the Series and its data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Monday       72\n",
       "Tuesday      75\n",
       "Wednesday    68\n",
       "Thursday     71\n",
       "Friday       73\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Task 1a code here...\n",
    "# daily_hight_temps = {\"Monday\": 72, \"Tuesday\": 75, \"Wednesday\": 68, \"Thursday\": 71, \"Friday\": 73}\n",
    "temperatures = pd.Series([72, 75, 68, 71, 73], \n",
    "                             index=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])\n",
    "\n",
    "#temperatures = pd.Series(daily_hight_temps) \n",
    "#print(temperatures)\n",
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Task 1b: Create a DataFrame from a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Create a DataFrame called `products` with the following data:\n",
    "\n",
    "| product | price | quantity |\n",
    "|---------|-------|----------|\n",
    "| Widget  | 19.99 | 100 |\n",
    "| Gadget  | 24.99 | 75 |\n",
    "| Doohickey | 12.49 | 150 |\n",
    "\n",
    "Use a dictionary where keys are column names and values are lists. Print the DataFrame and report its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: \n",
      "     product  price  quantity\n",
      "0     Widget  19.99       100\n",
      "1     Gadget  24.99        75\n",
      "2  Doohickey  12.49       150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1b code here...\n",
    "products = pd.DataFrame({\n",
    "         'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "         'price': [19.99, 24.99, 12.49],\n",
    "         'quantity': [100, 75, 150]\n",
    "})\n",
    "print(\"Products: \")\n",
    "print(products)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Task 1c: Access DataFrame Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Using the `products` DataFrame from Task 1b, extract and print:\n",
    "\n",
    "1. The `price` column as a Series\n",
    "2. The `product` and `quantity` columns as a DataFrame (using a list of column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Widget</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gadget</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doohickey</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product  quantity\n",
       "0     Widget       100\n",
       "1     Gadget        75\n",
       "2  Doohickey       150"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1c code here...\n",
    "products['price']  \n",
    "products[['product', 'quantity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer the following questions (briefly / concisely) in the markdown cell below:\n",
    "\n",
    "1. Data structure mapping: When you create a DataFrame from a dictionary (like in Task 1b), what do the dictionary keys become? What do the values become?\n",
    "2. Bracket notation: Why does `df['price']` return a Series, but `df[['price']]` return a DataFrame? What's the difference in what you're asking for?\n",
    "3. Index purpose: In Task 1a, you used day names as the index instead of default numbers (0, 1, 2...). When would a custom index like this be more useful than the default numeric index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Problem 1 Interpretation \n",
    "\n",
    "1. The dictionary _keys_ become columns and _values_ become rows.\n",
    "2. `df['price']` returns a Series and `df[['price']]` returns a DataFrame due to the different syntax used, which is *single* versus *double* brackets:\n",
    "    - When using *single* brackets in `df['price']`, the code is telling pandas to return the values of the columns with no **heading** for each column, but only integer values for labeling each row.\n",
    "    - When using *double* brackets in `df[['price']]`, the code is telling pandas to return the values of the columns with a **heading** for each column in addition to integer values for labeling each row.\n",
    "3. A custom index would be more useful possibly when a data analyst wants to index/access a row by a *name* instead of an *integer* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Problem 2: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Before starting this problem, make sure you are working in a copy of this file in the `my_repo` folder you created in HW2a. You must also have a copy of the file `202410-divvy-tripdata-100k.csv` in a subdirectory called `data`. That file structure is illustrated below.\n",
    "\n",
    "```text\n",
    "~/insy6500/my_repo\n",
    "└── homework\n",
    "    ├── data\n",
    "    │   └── 202410-divvy-tripdata-100k.csv\n",
    "    └── hw3b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Task 2a: Load and Understand Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Start by loading the data \"as-is\" to get a general understanding of the overall structure and how Pandas interprets it by default.\n",
    "\n",
    "Note on file paths: The provided code uses `Path` from Python's `pathlib` module to handle file paths. Path objects work consistently across operating systems (Windows uses backslashes `\\`, Mac/Linux use forward slashes `/`), automatically using the correct separator for your system. The provided code defines `csv_path` which should be used as the filename in your `pd.read_csv` to load the data file.\n",
    "\n",
    "1. Use `pd.read_csv` to load `csv_path` (provided below) without specifying any other arguments. Assign it to the variable `df_raw`.\n",
    "2. Use the methods we described in class to explore the shape, structure, types, etc. of the data. In particular, consider which columns represent dates or categories.\n",
    "3. Note the amount of memory used by the dataset. See the section on memory diagnostics in notebook 07a for appropriate code snippets using `memory_usage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237fe2de-4921-4d90-8e4e-534bb734f7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                 ride_id  rideable_type               started_at  \\\n",
       "0      67BB74BD7667BAB7  electric_bike  2024-09-30 23:12:01.622   \n",
       "1      5AF1AC3BA86ED58C  electric_bike  2024-09-30 23:19:25.409   \n",
       "2      7961DD2FC1280CDC   classic_bike  2024-09-30 23:32:24.672   \n",
       "3      2E16892DEEF4CC19   classic_bike  2024-09-30 23:42:11.207   \n",
       "4      AAF0220F819BEE01  electric_bike  2024-09-30 23:49:25.380   \n",
       "...                 ...            ...                      ...   \n",
       "99995  6D5AFF497514A788   classic_bike  2024-10-31 23:44:23.211   \n",
       "99996  527E9D2BDCAFFEC4   classic_bike  2024-10-31 23:44:45.948   \n",
       "99997  33A63439F82E7542   classic_bike  2024-10-31 23:50:31.160   \n",
       "99998  2BE6AF69988C197F   classic_bike  2024-10-31 23:53:02.355   \n",
       "99999  A925983EBD0E911E   classic_bike  2024-10-31 23:54:02.851   \n",
       "\n",
       "                      ended_at         start_station_name start_station_id  \\\n",
       "0      2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
       "1      2024-10-01 00:42:09.933                        NaN              NaN   \n",
       "2      2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
       "3      2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
       "4      2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
       "...                        ...                        ...              ...   \n",
       "99995  2024-10-31 23:49:31.022   Wells St & Evergreen Ave           904fde   \n",
       "99996  2024-10-31 23:50:47.698      Clark St & Newport St           1f2cb8   \n",
       "99997  2024-10-31 23:55:44.488  Damen Ave & Charleston St           929cd0   \n",
       "99998  2024-10-31 23:58:27.675   Lincoln Ave & Roscoe St*           a2de82   \n",
       "99999  2024-10-31 23:57:44.279     State St & Harrison St           f4980a   \n",
       "\n",
       "                    end_station_name end_station_id  start_lat  start_lng  \\\n",
       "0                                NaN            NaN  42.012342 -87.688243   \n",
       "1             Benson Ave & Church St         a10cf0  42.070000 -87.730000   \n",
       "2           LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798   \n",
       "3             Loomis St & Archer Ave         896337  41.895954 -87.667728   \n",
       "4                  900 W Harrison St         11da85  41.874754 -87.649807   \n",
       "...                              ...            ...        ...        ...   \n",
       "99995        Wells St & Institute Pl         ef812f  41.906724 -87.634830   \n",
       "99996   Southport Ave & Waveland Ave         9611f6  41.944540 -87.654678   \n",
       "99997         Damen Ave & Pierce Ave         fbf054  41.920082 -87.677855   \n",
       "99998       Leavitt St & Belmont Ave         a7b478  41.943350 -87.670668   \n",
       "99999  Financial Pl & Ida B Wells Dr         9cda45  41.874053 -87.627716   \n",
       "\n",
       "         end_lat    end_lng member_casual  \n",
       "0      41.970000 -87.650000        casual  \n",
       "1      42.048214 -87.683485        casual  \n",
       "2      41.890762 -87.631697        member  \n",
       "3      41.841633 -87.657435        casual  \n",
       "4      41.874754 -87.649807        member  \n",
       "...          ...        ...           ...  \n",
       "99995  41.897380 -87.634420        casual  \n",
       "99996  41.948226 -87.664071        member  \n",
       "99997  41.909396 -87.677692        casual  \n",
       "99998  41.939354 -87.683282        member  \n",
       "99999  41.875024 -87.633094        member  \n",
       "\n",
       "[100000 rows x 13 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# load and explore the data below (create additional code / markdown cells as necessary)\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "df_raw.info # Displays the information/data of all the columns\n",
    "#df_raw.dtypes\n",
    "#df_raw.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d3fc5ae7-be60-496c-a8df-e11d1ee366f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage by column:\n",
      "Index                     132\n",
      "ride_id               7300000\n",
      "rideable_type         6950493\n",
      "started_at            8000000\n",
      "ended_at              8000000\n",
      "start_station_name    7549653\n",
      "start_station_id      5978313\n",
      "end_station_name      7546619\n",
      "end_station_id        5974035\n",
      "start_lat              800000\n",
      "start_lng              800000\n",
      "end_lat                800000\n",
      "end_lng                800000\n",
      "member_casual         6300000\n",
      "dtype: int64\n",
      "\n",
      "Data Types: \n",
      "ride_id                object\n",
      "rideable_type          object\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "start_station_id       object\n",
      "end_station_name       object\n",
      "end_station_id         object\n",
      "start_lat             float64\n",
      "start_lng             float64\n",
      "end_lat               float64\n",
      "end_lng               float64\n",
      "member_casual          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#### import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# load and explore the data below (create additional code / markdown cells as necessary)\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "#df_raw.info() # Displays columns in row format and specifically shows only two characteristics of the columns \n",
    "#df_raw.describe()\n",
    "\n",
    "# Check memory usage by column\n",
    "print(\"Memory usage by column:\")\n",
    "print(df_raw.memory_usage(deep=True))\n",
    "\n",
    "print(f\"\\nData Types: \\n{df_raw.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Task 2b: Reload with Proper Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "1. Repeat step 2a.1 to reload the data. Use the `dtype` and `parse_dates` arguments to properly assign categorical and date types. Assign the result to the variable name `rides`.\n",
    "2. After loading, use `rides.info()` to confirm the type changes.\n",
    "3. Use `memory_usage` to compare the resulting size with that from step 2a.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrangement of data with proper data types: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   ride_id             100000 non-null  object        \n",
      " 1   rideable_type       100000 non-null  category      \n",
      " 2   started_at          100000 non-null  datetime64[ns]\n",
      " 3   ended_at            100000 non-null  datetime64[ns]\n",
      " 4   start_station_name  89623 non-null   category      \n",
      " 5   start_station_id    89623 non-null   category      \n",
      " 6   end_station_name    89485 non-null   category      \n",
      " 7   end_station_id      89485 non-null   category      \n",
      " 8   start_lat           100000 non-null  float64       \n",
      " 9   start_lng           100000 non-null  float64       \n",
      " 10  end_lat             99913 non-null   float64       \n",
      " 11  end_lng             99913 non-null   float64       \n",
      " 12  member_casual       100000 non-null  category      \n",
      "dtypes: category(6), datetime64[ns](2), float64(4), object(1)\n",
      "memory usage: 6.4+ MB\n",
      "\n",
      " Memory usage by column with raw data: \n",
      "Index                     132\n",
      "ride_id               7300000\n",
      "rideable_type         6950493\n",
      "started_at            8000000\n",
      "ended_at              8000000\n",
      "start_station_name    7549653\n",
      "start_station_id      5978313\n",
      "end_station_name      7546619\n",
      "end_station_id        5974035\n",
      "start_lat              800000\n",
      "start_lng              800000\n",
      "end_lat                800000\n",
      "end_lng                800000\n",
      "member_casual         6300000\n",
      "dtype: int64\n",
      "\n",
      "Memory usage by column with proper data types:\n",
      "Index                     132\n",
      "ride_id               7300000\n",
      "rideable_type          100247\n",
      "started_at             800000\n",
      "ended_at               800000\n",
      "start_station_name     300185\n",
      "start_station_id       285039\n",
      "end_station_name       300381\n",
      "end_station_id         285165\n",
      "start_lat              800000\n",
      "start_lng              800000\n",
      "end_lat                800000\n",
      "end_lng                800000\n",
      "member_casual          100126\n",
      "dtype: int64\n",
      "\n",
      "Total memory usage with raw data:\n",
      "63.70 MB\n",
      "\n",
      "Total memory usage with proper data types:\n",
      "12.85 MB\n"
     ]
    }
   ],
   "source": [
    "# task 2b code here...\n",
    "df_raw = pd.read_csv(csv_path)    \n",
    "\n",
    "# dtypes_data = df_raw.dtypes\n",
    "# df_raw.dtypes\n",
    "#df_raw.info()\n",
    "\n",
    "dtype_dict={ 'ride_id': 'object',\n",
    "             'rideable_type': 'category',\n",
    "             'start_station_name': 'category',\n",
    "             'start_station_id': 'category',\n",
    "             'end_station_name': 'category',\n",
    "             'end_station_id': 'category',\n",
    "             'member_casual': 'category'                \n",
    "           }  \n",
    "                                \n",
    "rides = pd.read_csv(csv_path, \n",
    "                    dtype=dtype_dict,\n",
    "                    parse_dates=['started_at', 'ended_at']) \n",
    "\n",
    "# print(\"Rides: \")\n",
    "# print(rides) \n",
    "print(\"arrangement of data with proper data types: \\n\")\n",
    "rides.info()  \n",
    "\n",
    "# Check memory usage by column\n",
    "print(\"\\n Memory usage by column with raw data: \")\n",
    "print(df_raw.memory_usage(deep=True))\n",
    "\n",
    "# Check memory usage by column\n",
    "print(\"\\nMemory usage by column with proper data types:\")\n",
    "print(rides.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nTotal memory usage with raw data:\")\n",
    "print(f\"{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nTotal memory usage with proper data types:\")\n",
    "print(f\"{rides.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Task 2c: Explore Structure and Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Using the `rides` DataFrame from Task 2b:\n",
    "\n",
    "1. Determine the range of starting dates in the dataframe using the `min` and `max` methods.\n",
    "2. Count the number of missing values in each column. See the section of the same name in lecture 06b.\n",
    "3. Convert the Series from step 2 to a DataFrame using `.to_frame(name='count')`, then add a column called 'percentage' that calculates the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dates: \n",
      "0       2024-09-30 23:12:01.622\n",
      "1       2024-09-30 23:19:25.409\n",
      "2       2024-09-30 23:32:24.672\n",
      "3       2024-09-30 23:42:11.207\n",
      "4       2024-09-30 23:49:25.380\n",
      "                  ...          \n",
      "99995   2024-10-31 23:44:23.211\n",
      "99996   2024-10-31 23:44:45.948\n",
      "99997   2024-10-31 23:50:31.160\n",
      "99998   2024-10-31 23:53:02.355\n",
      "99999   2024-10-31 23:54:02.851\n",
      "Name: started_at, Length: 100000, dtype: datetime64[ns]\n",
      "\n",
      "Minumum date: \n",
      "2024-09-30 23:12:01.622000\n",
      "\n",
      "Maximum date: \n",
      "2024-10-31 23:54:02.851000\n",
      "\n",
      "Number of missing values: \n",
      "ride_id                   0\n",
      "rideable_type             0\n",
      "started_at                0\n",
      "ended_at                  0\n",
      "start_station_name    10377\n",
      "start_station_id      10377\n",
      "end_station_name      10515\n",
      "end_station_id        10515\n",
      "start_lat                 0\n",
      "start_lng                 0\n",
      "end_lat                  87\n",
      "end_lng                  87\n",
      "member_casual             0\n",
      "dtype: int64\n",
      "                    count  percentage\n",
      "ride_id                 0       0.000\n",
      "rideable_type           0       0.000\n",
      "started_at              0       0.000\n",
      "ended_at                0       0.000\n",
      "start_station_name  10377      10.377\n",
      "start_station_id    10377      10.377\n",
      "end_station_name    10515      10.515\n",
      "end_station_id      10515      10.515\n",
      "start_lat               0       0.000\n",
      "start_lng               0       0.000\n",
      "end_lat                87       0.087\n",
      "end_lng                87       0.087\n",
      "member_casual           0       0.000\n"
     ]
    }
   ],
   "source": [
    "# task 2c code here...\n",
    "range_startDates = rides['started_at']\n",
    "\n",
    "#print(range_startDates) \n",
    "print(f\"Start dates: \\n{range_startDates}\\n\")\n",
    "#print(range_startDates.min) \n",
    "\n",
    "Minimum_date = range_startDates.min()\n",
    "Maximum_date = range_startDates.max()\n",
    "print(f\"Minumum date: \\n{Minimum_date}\\n\")\n",
    "print(f\"Maximum date: \\n{Maximum_date}\\n\")\n",
    "\n",
    "#print(rides.isna())\n",
    "Num_NaN = rides.isna().sum()\n",
    "print(f\"Number of missing values: \\n{Num_NaN}\")\n",
    "\n",
    "Num_NaN_DateFrame = Num_NaN.to_frame(name='count')\n",
    "Num_NaN_DateFrame['percentage'] = ((Num_NaN_DateFrame['count'] / len(rides)) * 100)\n",
    "# Num_NaN_DateFrame['new_column'] -> standard way to add a new column in pandas \n",
    "\n",
    "print(Num_NaN_DateFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Task 2d: Create Trip Duration Column and Set Index\n",
    "\n",
    "Before setting the index, create a derived column for trip duration:\n",
    "\n",
    "1. Calculate trip_duration_min by subtracting `started_at` from `ended_at`, then converting to minutes using `.dt.total_seconds() / 60`\n",
    "3. Display basic statistics (min, max, mean) for the new column using `.describe()`\n",
    "4. Show the first few rows with `started_at`, `ended_at`, and `trip_duration_min` to verify the calculation\n",
    "5. Set `started_at` as the DataFrame's index. Verify the change by printing the index and displaying the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip duration in minutes: \n",
      "0        67.984200\n",
      "1        82.742067\n",
      "2        50.899583\n",
      "3        28.093733\n",
      "4        17.034933\n",
      "           ...    \n",
      "99995     5.130183\n",
      "99996     6.029167\n",
      "99997     5.222133\n",
      "99998     5.422000\n",
      "99999     3.690467\n",
      "Length: 100000, dtype: float64\n",
      "\n",
      "Basic Statistics: \n",
      "count    100000.000000\n",
      "mean         16.144576\n",
      "std          52.922539\n",
      "min           0.006533\n",
      "25%           5.489271\n",
      "50%           9.423592\n",
      "75%          16.407171\n",
      "max        1499.949717\n",
      "Name: trip_duration, dtype: float64\n",
      "\n",
      "                ride_id  rideable_type              started_at  \\\n",
      "0      67BB74BD7667BAB7  electric_bike 2024-09-30 23:12:01.622   \n",
      "1      5AF1AC3BA86ED58C  electric_bike 2024-09-30 23:19:25.409   \n",
      "2      7961DD2FC1280CDC   classic_bike 2024-09-30 23:32:24.672   \n",
      "3      2E16892DEEF4CC19   classic_bike 2024-09-30 23:42:11.207   \n",
      "4      AAF0220F819BEE01  electric_bike 2024-09-30 23:49:25.380   \n",
      "...                 ...            ...                     ...   \n",
      "99995  6D5AFF497514A788   classic_bike 2024-10-31 23:44:23.211   \n",
      "99996  527E9D2BDCAFFEC4   classic_bike 2024-10-31 23:44:45.948   \n",
      "99997  33A63439F82E7542   classic_bike 2024-10-31 23:50:31.160   \n",
      "99998  2BE6AF69988C197F   classic_bike 2024-10-31 23:53:02.355   \n",
      "99999  A925983EBD0E911E   classic_bike 2024-10-31 23:54:02.851   \n",
      "\n",
      "                     ended_at         start_station_name start_station_id  \\\n",
      "0     2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
      "1     2024-10-01 00:42:09.933                        NaN              NaN   \n",
      "2     2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
      "3     2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
      "4     2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
      "...                       ...                        ...              ...   \n",
      "99995 2024-10-31 23:49:31.022   Wells St & Evergreen Ave           904fde   \n",
      "99996 2024-10-31 23:50:47.698      Clark St & Newport St           1f2cb8   \n",
      "99997 2024-10-31 23:55:44.488  Damen Ave & Charleston St           929cd0   \n",
      "99998 2024-10-31 23:58:27.675   Lincoln Ave & Roscoe St*           a2de82   \n",
      "99999 2024-10-31 23:57:44.279     State St & Harrison St           f4980a   \n",
      "\n",
      "                    end_station_name end_station_id  start_lat  start_lng  \\\n",
      "0                                NaN            NaN  42.012342 -87.688243   \n",
      "1             Benson Ave & Church St         a10cf0  42.070000 -87.730000   \n",
      "2           LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798   \n",
      "3             Loomis St & Archer Ave         896337  41.895954 -87.667728   \n",
      "4                  900 W Harrison St         11da85  41.874754 -87.649807   \n",
      "...                              ...            ...        ...        ...   \n",
      "99995        Wells St & Institute Pl         ef812f  41.906724 -87.634830   \n",
      "99996   Southport Ave & Waveland Ave         9611f6  41.944540 -87.654678   \n",
      "99997         Damen Ave & Pierce Ave         fbf054  41.920082 -87.677855   \n",
      "99998       Leavitt St & Belmont Ave         a7b478  41.943350 -87.670668   \n",
      "99999  Financial Pl & Ida B Wells Dr         9cda45  41.874053 -87.627716   \n",
      "\n",
      "         end_lat    end_lng member_casual  trip_duration  \n",
      "0      41.970000 -87.650000        casual      67.984200  \n",
      "1      42.048214 -87.683485        casual      82.742067  \n",
      "2      41.890762 -87.631697        member      50.899583  \n",
      "3      41.841633 -87.657435        casual      28.093733  \n",
      "4      41.874754 -87.649807        member      17.034933  \n",
      "...          ...        ...           ...            ...  \n",
      "99995  41.897380 -87.634420        casual       5.130183  \n",
      "99996  41.948226 -87.664071        member       6.029167  \n",
      "99997  41.909396 -87.677692        casual       5.222133  \n",
      "99998  41.939354 -87.683282        member       5.422000  \n",
      "99999  41.875024 -87.633094        member       3.690467  \n",
      "\n",
      "[100000 rows x 14 columns]\n",
      "                                  ride_id  rideable_type  \\\n",
      "started_at                                                 \n",
      "2024-09-30 23:12:01.622  67BB74BD7667BAB7  electric_bike   \n",
      "2024-09-30 23:19:25.409  5AF1AC3BA86ED58C  electric_bike   \n",
      "2024-09-30 23:32:24.672  7961DD2FC1280CDC   classic_bike   \n",
      "2024-09-30 23:42:11.207  2E16892DEEF4CC19   classic_bike   \n",
      "2024-09-30 23:49:25.380  AAF0220F819BEE01  electric_bike   \n",
      "...                                   ...            ...   \n",
      "2024-10-31 23:44:23.211  6D5AFF497514A788   classic_bike   \n",
      "2024-10-31 23:44:45.948  527E9D2BDCAFFEC4   classic_bike   \n",
      "2024-10-31 23:50:31.160  33A63439F82E7542   classic_bike   \n",
      "2024-10-31 23:53:02.355  2BE6AF69988C197F   classic_bike   \n",
      "2024-10-31 23:54:02.851  A925983EBD0E911E   classic_bike   \n",
      "\n",
      "                                       ended_at         start_station_name  \\\n",
      "started_at                                                                   \n",
      "2024-09-30 23:12:01.622 2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave   \n",
      "2024-09-30 23:19:25.409 2024-10-01 00:42:09.933                        NaN   \n",
      "2024-09-30 23:32:24.672 2024-10-01 00:23:18.647     St. Clair St & Erie St   \n",
      "2024-09-30 23:42:11.207 2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave   \n",
      "2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          900 W Harrison St   \n",
      "...                                         ...                        ...   \n",
      "2024-10-31 23:44:23.211 2024-10-31 23:49:31.022   Wells St & Evergreen Ave   \n",
      "2024-10-31 23:44:45.948 2024-10-31 23:50:47.698      Clark St & Newport St   \n",
      "2024-10-31 23:50:31.160 2024-10-31 23:55:44.488  Damen Ave & Charleston St   \n",
      "2024-10-31 23:53:02.355 2024-10-31 23:58:27.675   Lincoln Ave & Roscoe St*   \n",
      "2024-10-31 23:54:02.851 2024-10-31 23:57:44.279     State St & Harrison St   \n",
      "\n",
      "                        start_station_id               end_station_name  \\\n",
      "started_at                                                                \n",
      "2024-09-30 23:12:01.622           bdd4c3                            NaN   \n",
      "2024-09-30 23:19:25.409              NaN         Benson Ave & Church St   \n",
      "2024-09-30 23:32:24.672           9c619a       LaSalle St & Illinois St   \n",
      "2024-09-30 23:42:11.207           72a04d         Loomis St & Archer Ave   \n",
      "2024-09-30 23:49:25.380           11da85              900 W Harrison St   \n",
      "...                                  ...                            ...   \n",
      "2024-10-31 23:44:23.211           904fde        Wells St & Institute Pl   \n",
      "2024-10-31 23:44:45.948           1f2cb8   Southport Ave & Waveland Ave   \n",
      "2024-10-31 23:50:31.160           929cd0         Damen Ave & Pierce Ave   \n",
      "2024-10-31 23:53:02.355           a2de82       Leavitt St & Belmont Ave   \n",
      "2024-10-31 23:54:02.851           f4980a  Financial Pl & Ida B Wells Dr   \n",
      "\n",
      "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
      "started_at                                                                \n",
      "2024-09-30 23:12:01.622            NaN  42.012342 -87.688243  41.970000   \n",
      "2024-09-30 23:19:25.409         a10cf0  42.070000 -87.730000  42.048214   \n",
      "2024-09-30 23:32:24.672         fbd1ad  41.894345 -87.622798  41.890762   \n",
      "2024-09-30 23:42:11.207         896337  41.895954 -87.667728  41.841633   \n",
      "2024-09-30 23:49:25.380         11da85  41.874754 -87.649807  41.874754   \n",
      "...                                ...        ...        ...        ...   \n",
      "2024-10-31 23:44:23.211         ef812f  41.906724 -87.634830  41.897380   \n",
      "2024-10-31 23:44:45.948         9611f6  41.944540 -87.654678  41.948226   \n",
      "2024-10-31 23:50:31.160         fbf054  41.920082 -87.677855  41.909396   \n",
      "2024-10-31 23:53:02.355         a7b478  41.943350 -87.670668  41.939354   \n",
      "2024-10-31 23:54:02.851         9cda45  41.874053 -87.627716  41.875024   \n",
      "\n",
      "                           end_lng member_casual  trip_duration  \n",
      "started_at                                                       \n",
      "2024-09-30 23:12:01.622 -87.650000        casual      67.984200  \n",
      "2024-09-30 23:19:25.409 -87.683485        casual      82.742067  \n",
      "2024-09-30 23:32:24.672 -87.631697        member      50.899583  \n",
      "2024-09-30 23:42:11.207 -87.657435        casual      28.093733  \n",
      "2024-09-30 23:49:25.380 -87.649807        member      17.034933  \n",
      "...                            ...           ...            ...  \n",
      "2024-10-31 23:44:23.211 -87.634420        casual       5.130183  \n",
      "2024-10-31 23:44:45.948 -87.664071        member       6.029167  \n",
      "2024-10-31 23:50:31.160 -87.677692        casual       5.222133  \n",
      "2024-10-31 23:53:02.355 -87.683282        member       5.422000  \n",
      "2024-10-31 23:54:02.851 -87.633094        member       3.690467  \n",
      "\n",
      "[100000 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# task 2d code here...\n",
    "rides_DataFrame = pd.DataFrame(rides, columns=['ride_id',\n",
    "                                               'rideable_type',\n",
    "                                               'started_at',\n",
    "                                               'ended_at',\n",
    "                                               'start_station_name',\n",
    "                                               'start_station_id',\n",
    "                                               'end_station_name',\n",
    "                                               'end_station_id',\n",
    "                                               'start_lat',\n",
    "                                               'start_lng',\n",
    "                                               'end_lat',\n",
    "                                               'end_lng',\n",
    "                                               'member_casual'])\n",
    "\n",
    "trip_duration_sec = rides_DataFrame['ended_at'] - rides_DataFrame['started_at']\n",
    "trip_duration_min = trip_duration_sec.dt.total_seconds() / 60\n",
    "rides_DataFrame['trip_duration'] = trip_duration_min \n",
    "\n",
    "print(f\"Trip duration in minutes: \\n{trip_duration_min}\\n\") \n",
    "#print(rides) \n",
    "\n",
    "rides_statistics = rides_DataFrame['trip_duration'].describe()\n",
    "print(f\"Basic Statistics: \\n{rides_statistics}\\n\")\n",
    "\n",
    "print(rides_DataFrame)\n",
    "#print(f\"\\nData Types: \\n{df_raw.dtypes}\")\n",
    "\n",
    "#df_raw.info\n",
    "rides_DataFrame_date = rides_DataFrame.set_index('started_at')\n",
    "print(rides_DataFrame_date) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Reflect on problem 2 and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. What types did Pandas assign to `started_at` and `member_casual` in Task 2a? Why might these defaults be problematic?\n",
    "2. Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?\n",
    "3. Explain in your own words what method chaining is, what `df.isna().sum()` does and how it works.\n",
    "4. Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Problem 2 Interpretation \n",
    "1. Pandas assigned the *data type*, `object` to both `started_at` and `member_casual`.\n",
    "    - These default *data types* could be problematic because the two items are actually different types of information/data.   \n",
    "        - `started_at` actually includes dates and times, versus `member_casua` includes two options, which describe/label the type of rider as *member* or *casual*  \n",
    "1. The directions in HW3a explained that commits are given a unique ID; therefore, I would think that the station ID fields are randomly generated. HW3a further describes them as a fingerprint.  \n",
    "1. Method chaining is using `df.isna().sum()` where:  \n",
    "   - The `isna` method finds all the non-number, NaN, values and labels them as `True` or `False`. Since Python uses True and False as labels for the binary values `1` and `0`, the `sum` function can be used to count the number of `True` values, which are the NaN values.\n",
    "1. Depending on how the name of the location was found, there might not have been a specific location name to correlate with the coordinates. \n",
    "    - Another possible explanation is that there was a technical error in the software, in which pandas skipped over the data when reading the CSV file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Compare memory usage results in 2a.3 and 2b.3. What caused the change? Why are these numbers different from what is reported at the bottom of `df.info()`? Which should you use if data size is a concern?\n",
    "\n",
    "Working with DataFrames typically requires 5-10x the dataset size in available RAM. On a system with 16GB, assuming about 30% overhead from the operating system and other programs, what range of dataset sizes would be safely manageable? Calculate using both 5x (optimistic) and 10x (conservative) multipliers, then explain which you'd recommend for reliable work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Total memory usage with raw data: 63.70 MB  \n",
    "Total memory usage with proper data types: 12.85 MB\n",
    "- The format of the raw data occupied much more memory than a format with the proper data types.\n",
    "    - I believe this could be due to the raw data being displayed in a format where column items were paired with appropriate/correct data types according to their type of value, which in this case is a float, category, object, and datetime.\n",
    "    - If data size is a concern, the format for proper data types should be used\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Problem 3: Filtering and Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Task 3a: Boolean Indexing and Membership Testing\n",
    "\n",
    "Use boolean indexing and the `isin()` method to answer these questions:\n",
    "\n",
    "1. How many trips were taken by *members* using *electric bikes*? Use `&` to combine conditions.\n",
    "2. What percentage of all trips does this represent?\n",
    "3. How many trips started at any of these three stations: \"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", or \"Kingsbury St & Kinzie St\"? Use `isin()`.\n",
    "\n",
    "Note: Remember to use parentheses around each condition when combining with `&`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trips taken by members using electric bikes: 33121\n",
      "\n",
      "started_at\n",
      "2024-09-30 23:12:01.622    electric_bike\n",
      "2024-09-30 23:19:25.409    electric_bike\n",
      "2024-09-30 23:32:24.672     classic_bike\n",
      "2024-09-30 23:42:11.207     classic_bike\n",
      "2024-09-30 23:49:25.380    electric_bike\n",
      "                               ...      \n",
      "2024-10-31 23:44:23.211     classic_bike\n",
      "2024-10-31 23:44:45.948     classic_bike\n",
      "2024-10-31 23:50:31.160     classic_bike\n",
      "2024-10-31 23:53:02.355     classic_bike\n",
      "2024-10-31 23:54:02.851     classic_bike\n",
      "Name: rideable_type, Length: 100000, dtype: category\n",
      "Categories (2, object): ['classic_bike', 'electric_bike']\n",
      "\n",
      "Trips taken by members using bikes: 100000\n",
      "\n",
      "Percent of trips used with electric bikes: 33.121%\n",
      "\n",
      "                                  ride_id  rideable_type  \\\n",
      "started_at                                                 \n",
      "2024-10-01 11:00:12.236  ADC5550BDA0B810A   classic_bike   \n",
      "2024-10-01 11:41:12.895  1DCB2A400A2A4506   classic_bike   \n",
      "2024-10-01 11:46:13.633  DAC4064243C365A1   classic_bike   \n",
      "2024-10-01 12:18:07.209  322056C69E4F66DA   classic_bike   \n",
      "2024-10-01 13:07:58.878  14D5EDCB3B387166   classic_bike   \n",
      "...                                   ...            ...   \n",
      "2024-10-31 14:15:12.178  304B200783F743CE   classic_bike   \n",
      "2024-10-31 14:40:57.001  5C9AA6C955AB4000   classic_bike   \n",
      "2024-10-31 14:48:22.767  AF48713BF040421F   classic_bike   \n",
      "2024-10-31 17:04:01.011  86077E303BC967DA  electric_bike   \n",
      "2024-10-31 17:24:56.394  DA5B5FAC7FB28058   classic_bike   \n",
      "\n",
      "                                       ended_at  \\\n",
      "started_at                                        \n",
      "2024-10-01 11:00:12.236 2024-10-01 11:10:11.585   \n",
      "2024-10-01 11:41:12.895 2024-10-01 12:12:16.385   \n",
      "2024-10-01 11:46:13.633 2024-10-01 12:01:05.340   \n",
      "2024-10-01 12:18:07.209 2024-10-01 12:48:26.814   \n",
      "2024-10-01 13:07:58.878 2024-10-01 13:27:41.410   \n",
      "...                                         ...   \n",
      "2024-10-31 14:15:12.178 2024-10-31 14:59:35.001   \n",
      "2024-10-31 14:40:57.001 2024-10-31 14:45:42.300   \n",
      "2024-10-31 14:48:22.767 2024-10-31 14:57:49.795   \n",
      "2024-10-31 17:04:01.011 2024-10-31 17:10:20.110   \n",
      "2024-10-31 17:24:56.394 2024-10-31 18:16:41.731   \n",
      "\n",
      "                                        start_station_name start_station_id  \\\n",
      "started_at                                                                    \n",
      "2024-10-01 11:00:12.236  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-01 11:41:12.895  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-01 11:46:13.633  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-01 12:18:07.209  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-01 13:07:58.878  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "...                                                    ...              ...   \n",
      "2024-10-31 14:15:12.178  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-31 14:40:57.001  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-31 14:48:22.767  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-31 17:04:01.011  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "2024-10-31 17:24:56.394  DuSable Lake Shore Dr & Monroe St           8ba9b1   \n",
      "\n",
      "                                               end_station_name  \\\n",
      "started_at                                                        \n",
      "2024-10-01 11:00:12.236                   Streeter Dr/Grand Ave   \n",
      "2024-10-01 11:41:12.895                          Dusable Harbor   \n",
      "2024-10-01 11:46:13.633                            Field Museum   \n",
      "2024-10-01 12:18:07.209                          Dusable Harbor   \n",
      "2024-10-01 13:07:58.878                       Adler Planetarium   \n",
      "...                                                         ...   \n",
      "2024-10-31 14:15:12.178  Griffin Museum of Science and Industry   \n",
      "2024-10-31 14:40:57.001       DuSable Lake Shore Dr & Monroe St   \n",
      "2024-10-31 14:48:22.767                 Streeter Dr & Grand Ave   \n",
      "2024-10-31 17:04:01.011                 State St & Van Buren St   \n",
      "2024-10-31 17:24:56.394                    Broadway & Ridge Ave   \n",
      "\n",
      "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
      "started_at                                                                \n",
      "2024-10-01 11:00:12.236         69c4ae  41.880958 -87.616743  41.892401   \n",
      "2024-10-01 11:41:12.895         13f096  41.880958 -87.616743  41.886976   \n",
      "2024-10-01 11:46:13.633         80bb72  41.880958 -87.616743  41.865312   \n",
      "2024-10-01 12:18:07.209         13f096  41.880958 -87.616743  41.886976   \n",
      "2024-10-01 13:07:58.878         64d7f5  41.880958 -87.616743  41.866095   \n",
      "...                                ...        ...        ...        ...   \n",
      "2024-10-31 14:15:12.178         9f4614  41.880958 -87.616743  41.791728   \n",
      "2024-10-31 14:40:57.001         8ba9b1  41.880958 -87.616743  41.880958   \n",
      "2024-10-31 14:48:22.767         c3583f  41.880958 -87.616743  41.892278   \n",
      "2024-10-31 17:04:01.011         ed54b8  41.880958 -87.616743  41.877181   \n",
      "2024-10-31 17:24:56.394         37253c  41.880958 -87.616743  41.984045   \n",
      "\n",
      "                           end_lng member_casual  trip_duration  \n",
      "started_at                                                       \n",
      "2024-10-01 11:00:12.236 -87.612388        casual       9.989150  \n",
      "2024-10-01 11:41:12.895 -87.612813        casual      31.058167  \n",
      "2024-10-01 11:46:13.633 -87.617867        casual      14.861783  \n",
      "2024-10-01 12:18:07.209 -87.612813        casual      30.326750  \n",
      "2024-10-01 13:07:58.878 -87.607267        casual      19.708867  \n",
      "...                            ...           ...            ...  \n",
      "2024-10-31 14:15:12.178 -87.583945        member      44.380383  \n",
      "2024-10-31 14:40:57.001 -87.616743        casual       4.754983  \n",
      "2024-10-31 14:48:22.767 -87.612043        casual       9.450467  \n",
      "2024-10-31 17:04:01.011 -87.627844        casual       6.318317  \n",
      "2024-10-31 17:24:56.394 -87.660274        member      51.755617  \n",
      "\n",
      "[809 rows x 13 columns]\n",
      "\n",
      "Number of trips started at 'DuSable Lake Shore Dr & Monroe St': 809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3a code here...\n",
    "e_bikes = rides[(rides['member_casual'] == 'member') & (rides['rideable_type'] == 'electric_bike')]\n",
    "num_e_bikes = len(e_bikes) \n",
    "\n",
    "bikes = rides['rideable_type']\n",
    "# Could technically use 'bikes' but using rides also returns the amount of rows in each column \n",
    "num_bikes = len(rides) \n",
    "\n",
    "num_trips_values = rides[rides['start_station_name'].isin(['DuSable Lake Shore Dr & Monroe St'])] \n",
    "num_trips = len(num_trips_values)\n",
    "\n",
    "#print(e_bikes)\n",
    "print(f\"\\nTrips taken by members using electric bikes: {num_e_bikes}\\n\")\n",
    "print(bikes)\n",
    "print(f\"\\nTrips taken by members using bikes: {num_bikes}\\n\")\n",
    "\n",
    "print(f\"Percent of trips used with electric bikes: {(num_e_bikes/num_bikes)*100}%\\n\")\n",
    "\n",
    "print(num_trips_values)\n",
    "print(f\"\\nNumber of trips started at 'DuSable Lake Shore Dr & Monroe St': {num_trips}\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Task 3b: Create Derived Columns from Datetime+\n",
    "\n",
    "Add two categorical columns to the rides DataFrame based on trip start time:\n",
    "\n",
    "1. `is_weekend`: Boolean column that is True for Saturday/Sunday trips. Use .dt.dayofweek on the index (Monday=0, Sunday=6).\n",
    "2. `time_of_day`: String categories based on start hour:\n",
    "   - \"Morning Rush\" if hour is 7, 8, or 9\n",
    "   - \"Evening Rush\" if hour is 16, 17, or 18\n",
    "   - \"Midday\" for all other hours\n",
    "\n",
    "For step 2, initialize the column to \"Midday\", then use .loc[mask, 'time_of_day'] with boolean masks to assign rush hour categories. Extract hour using .dt.hour on the index.\n",
    "\n",
    "After creating both columns, use value_counts() on time_of_day to show the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb8c48-589f-4f76-a6b9-05256940a4bc",
   "metadata": {},
   "source": [
    "#### Brainstorming \n",
    "- Creating *categories* in the column `time_of_day` and assigning them the appropiate start hours\n",
    "    - Use `.loc[mask, 'time_of_day']` **but first**:\n",
    "      - Need to use `.dt.hour` to extract the hour values from the column `'started_at'` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "99995    3\n",
      "99996    3\n",
      "99997    3\n",
      "99998    3\n",
      "99999    3\n",
      "Name: started_at, Length: 100000, dtype: int32\n",
      "time_of_day\n",
      "Midday          55912\n",
      "Evening Rush    28218\n",
      "Morning Rush    15870\n",
      "Name: count, dtype: int64\n",
      "                ride_id  rideable_type              started_at  \\\n",
      "0      67BB74BD7667BAB7  electric_bike 2024-09-30 23:12:01.622   \n",
      "1      5AF1AC3BA86ED58C  electric_bike 2024-09-30 23:19:25.409   \n",
      "2      7961DD2FC1280CDC   classic_bike 2024-09-30 23:32:24.672   \n",
      "3      2E16892DEEF4CC19   classic_bike 2024-09-30 23:42:11.207   \n",
      "4      AAF0220F819BEE01  electric_bike 2024-09-30 23:49:25.380   \n",
      "...                 ...            ...                     ...   \n",
      "99995  6D5AFF497514A788   classic_bike 2024-10-31 23:44:23.211   \n",
      "99996  527E9D2BDCAFFEC4   classic_bike 2024-10-31 23:44:45.948   \n",
      "99997  33A63439F82E7542   classic_bike 2024-10-31 23:50:31.160   \n",
      "99998  2BE6AF69988C197F   classic_bike 2024-10-31 23:53:02.355   \n",
      "99999  A925983EBD0E911E   classic_bike 2024-10-31 23:54:02.851   \n",
      "\n",
      "                     ended_at         start_station_name start_station_id  \\\n",
      "0     2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
      "1     2024-10-01 00:42:09.933                        NaN              NaN   \n",
      "2     2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
      "3     2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
      "4     2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
      "...                       ...                        ...              ...   \n",
      "99995 2024-10-31 23:49:31.022   Wells St & Evergreen Ave           904fde   \n",
      "99996 2024-10-31 23:50:47.698      Clark St & Newport St           1f2cb8   \n",
      "99997 2024-10-31 23:55:44.488  Damen Ave & Charleston St           929cd0   \n",
      "99998 2024-10-31 23:58:27.675   Lincoln Ave & Roscoe St*           a2de82   \n",
      "99999 2024-10-31 23:57:44.279     State St & Harrison St           f4980a   \n",
      "\n",
      "                    end_station_name end_station_id  start_lat  start_lng  \\\n",
      "0                                NaN            NaN  42.012342 -87.688243   \n",
      "1             Benson Ave & Church St         a10cf0  42.070000 -87.730000   \n",
      "2           LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798   \n",
      "3             Loomis St & Archer Ave         896337  41.895954 -87.667728   \n",
      "4                  900 W Harrison St         11da85  41.874754 -87.649807   \n",
      "...                              ...            ...        ...        ...   \n",
      "99995        Wells St & Institute Pl         ef812f  41.906724 -87.634830   \n",
      "99996   Southport Ave & Waveland Ave         9611f6  41.944540 -87.654678   \n",
      "99997         Damen Ave & Pierce Ave         fbf054  41.920082 -87.677855   \n",
      "99998       Leavitt St & Belmont Ave         a7b478  41.943350 -87.670668   \n",
      "99999  Financial Pl & Ida B Wells Dr         9cda45  41.874053 -87.627716   \n",
      "\n",
      "         end_lat    end_lng member_casual  trip_duration  is_weekend  \\\n",
      "0      41.970000 -87.650000        casual      67.984200       False   \n",
      "1      42.048214 -87.683485        casual      82.742067       False   \n",
      "2      41.890762 -87.631697        member      50.899583       False   \n",
      "3      41.841633 -87.657435        casual      28.093733       False   \n",
      "4      41.874754 -87.649807        member      17.034933       False   \n",
      "...          ...        ...           ...            ...         ...   \n",
      "99995  41.897380 -87.634420        casual       5.130183       False   \n",
      "99996  41.948226 -87.664071        member       6.029167       False   \n",
      "99997  41.909396 -87.677692        casual       5.222133       False   \n",
      "99998  41.939354 -87.683282        member       5.422000       False   \n",
      "99999  41.875024 -87.633094        member       3.690467       False   \n",
      "\n",
      "      time_of_day  \n",
      "0          Midday  \n",
      "1          Midday  \n",
      "2          Midday  \n",
      "3          Midday  \n",
      "4          Midday  \n",
      "...           ...  \n",
      "99995      Midday  \n",
      "99996      Midday  \n",
      "99997      Midday  \n",
      "99998      Midday  \n",
      "99999      Midday  \n",
      "\n",
      "[100000 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# Task 3b code here...\n",
    "\n",
    "#rides['day_of_week'] = rides['started_at'].dt.dayofweek\n",
    "day_of_week = rides_DataFrame['started_at'].dt.dayofweek\n",
    "print(day_of_week) \n",
    "\n",
    "rides_DataFrame['is_weekend'] = day_of_week.isin([5,6]) # the numbers for days are being listed under the 'is_weekend' column  \n",
    "\n",
    "rides_DataFrame['time_of_day'] = \"Midday\" # Initializing each value in this column to \"Midday\"\n",
    "hour_values = rides_DataFrame['started_at'].dt.hour # Extracting hour values from the 'started_at' column \n",
    "#print(hour_values) \n",
    "\n",
    "specific_hours_MR = hour_values.isin([7,8,9]) # Finding hour values: [7,8,9]\n",
    "rides_DataFrame.loc[specific_hours_MR, 'time_of_day'] = \"Morning Rush\" \n",
    "\n",
    "specific_hours_ER = hour_values.isin([16,17,18]) # Finding hour values: [16,17,18]\n",
    "rides_DataFrame.loc[specific_hours_ER, 'time_of_day'] = \"Evening Rush\" \n",
    "\n",
    "print(rides_DataFrame['time_of_day'].value_counts())\n",
    "\n",
    "#del rides['is_weekend']\n",
    "#del rides['day_of_week']\n",
    "\n",
    "print(rides_DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3a62e-79d4-4769-ab7b-29121aad6443",
   "metadata": {},
   "source": [
    "#### Task 3c: Complex Filtering with query()\n",
    "\n",
    "Use the `query()` method to find trips that meet **all** of these criteria:\n",
    "- Casual riders (not members)\n",
    "- Weekend trips  \n",
    "- Duration greater than 20 minutes\n",
    "- Electric bikes\n",
    "\n",
    "Report:\n",
    "1. How many trips match these criteria?\n",
    "2. What percentage of all trips do they represent?\n",
    "3. What is the average duration of these trips?\n",
    "\n",
    "Hint: Column names work directly in `query()` strings. Combine conditions with `and`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ride_id, rideable_type, started_at, ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat, end_lng, member_casual, trip_duration, is_weekend, time_of_day]\n",
      "Index: []\n",
      "Trips with the four criteria: \n",
      "      member_casual  is_weekend  trip_duration  rideable_type\n",
      "14332        casual        True      56.608250  electric_bike\n",
      "14349        casual        True      30.063433  electric_bike\n",
      "14361        casual        True     140.151717  electric_bike\n",
      "14369        casual        True     131.828583  electric_bike\n",
      "14456        casual        True      23.264000  electric_bike\n",
      "...             ...         ...            ...            ...\n",
      "88031        casual        True      25.337467  electric_bike\n",
      "88097        casual        True      22.346083  electric_bike\n",
      "88197        casual        True      81.179050  electric_bike\n",
      "88212        casual        True      23.110933  electric_bike\n",
      "88273        casual        True      23.472500  electric_bike\n",
      "\n",
      "[1501 rows x 4 columns]\n",
      "\n",
      "1501 trips match these trip criteria\n",
      "\n",
      "Percent of trips these criteria represent: 1.5010000000000001%\n",
      "\n",
      "Average duration of these trips: 40.37\n"
     ]
    }
   ],
   "source": [
    "# Task 3c code here...\n",
    "\n",
    "#is_weekend = rides_DataFrame['is_weekend'].isin([5,6])\n",
    "rides_tripCriteria = rides_DataFrame[rides_DataFrame['is_weekend'].isin([5,6])]\n",
    "print(rides_tripCriteria)\n",
    "trip_criteria = rides_DataFrame.query('member_casual == \"casual\" and is_weekend == True and trip_duration > 20 and rideable_type == \"electric_bike\"')\n",
    "trip_criteria_columns = trip_criteria[['member_casual', 'is_weekend', 'trip_duration', 'rideable_type']]\n",
    "num_trip_criteria = len(trip_criteria_columns)\n",
    "print(f\"Trips with the four criteria: \\n{trip_criteria_columns}\")\n",
    "#print(rides_DataFrame['is_weekend'].unique())\n",
    "#num_trip_criteria = len(trip_criteria)\n",
    "#print(trip_criteria)\n",
    "print(f\"\\n{num_trip_criteria} trips match these trip criteria\")  \n",
    "#print(num_trip_criteria)\n",
    "\n",
    "percent_trip_criteria = (num_trip_criteria/len(rides_DataFrame)) * 100\n",
    "#print(percent_trip_criteria)\n",
    "\n",
    "print(f\"\\nPercent of trips these criteria represent: {percent_trip_criteria}%\\n\")  \n",
    "\n",
    "#Length = len(trip_criteria_values['trip_duration'])\n",
    "#Sum = trip_criteria_vxalues['trip_duration'].sum()\n",
    "print(f\"Average duration of these trips: {trip_criteria['trip_duration'].mean():.2f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Task 3d: Explicit Indexing Practice\n",
    "\n",
    "Practice using `loc[]` and `iloc[]` for different selection tasks:\n",
    "\n",
    "1. Use `iloc[]` to select the first 10 trips, showing only `member_casual`, `rideable_type`, and `trip_duration_min` columns\n",
    "2. Use `loc[]` to select trips from October 15-17 (use date strings `'2024-10-15':'2024-10-17'`), showing the same three columns\n",
    "3. Count how many trips occurred during this date range\n",
    "\n",
    "Note: When using `iloc[]`, remember it's position-based (0-indexed). When using `loc[]` with the datetime index, you can slice using date strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6d219-0d19-4669-9fab-a85923c4c627",
   "metadata": {},
   "source": [
    "#### Brainstorming  \n",
    "1. Uing `iloc[]` to find the 1st 10 trips of the columns `member_casual`, `rideable_type`, and `trip_duration_min` \n",
    "    - Finding rows in certain columns ex.: `[row 1, columns 0 and 1]`\n",
    "    - `frame3.iloc[1, 0:2]`: **Remember**, 2 is exclusive!\n",
    "2. I had trouble finding the dates in these columns since the dates are rows\n",
    "   - I asked Cluade, and it reminded me that since the dates are rows, the dates need to be set as the index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten trips for the columns 'member_casual', 'rideable_type', 'trip_duration': \n",
      "\n",
      "                        member_casual  rideable_type  trip_duration\n",
      "started_at                                                         \n",
      "2024-09-30 23:12:01.622        casual  electric_bike      67.984200\n",
      "2024-09-30 23:19:25.409        casual  electric_bike      82.742067\n",
      "2024-09-30 23:32:24.672        member   classic_bike      50.899583\n",
      "2024-09-30 23:42:11.207        casual   classic_bike      28.093733\n",
      "2024-09-30 23:49:25.380        member  electric_bike      17.034933\n",
      "2024-09-30 23:49:40.016        member  electric_bike      13.009367\n",
      "2024-10-01 00:00:53.414        member   classic_bike       2.598817\n",
      "2024-10-01 00:05:44.954        member  electric_bike       0.013433\n",
      "2024-10-01 00:06:12.035        member  electric_bike      10.472933\n",
      "2024-10-01 00:10:03.646        member   classic_bike       7.825683\n",
      "\n",
      "Trips made in the date range '2024-10-15' to '2024-10-17': \n",
      "\n",
      "                        member_casual  rideable_type  trip_duration\n",
      "started_at                                                         \n",
      "2024-10-15 00:00:12.781        casual  electric_bike       2.804233\n",
      "2024-10-15 00:01:20.517        member  electric_bike      11.330867\n",
      "2024-10-15 00:05:24.811        member  electric_bike       1.868850\n",
      "2024-10-15 00:05:52.984        member  electric_bike       2.705083\n",
      "2024-10-15 00:06:18.819        member  electric_bike       1.600867\n",
      "...                               ...            ...            ...\n",
      "2024-10-17 23:45:48.739        member  electric_bike      14.703100\n",
      "2024-10-17 23:47:35.040        member  electric_bike      13.049867\n",
      "2024-10-17 23:55:34.112        member   classic_bike       3.795400\n",
      "2024-10-17 23:56:14.464        member  electric_bike      11.937217\n",
      "2024-10-17 23:59:38.103        casual   classic_bike       5.266433\n",
      "\n",
      "[7235 rows x 3 columns]\n",
      "\n",
      "Number of trips made during the date range '2024-10-15' to '2024-10-17': 7235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3d code here...\n",
    "#rides_DataFrame_date = rides_DataFrame.set_index('started_at')\n",
    "three_Columns = rides_DataFrame_date[['member_casual', 'rideable_type', 'trip_duration']]\n",
    "#print(three_Columns) \n",
    "firstTen_trips = three_Columns.iloc[:10]  \n",
    "print(f\"First ten trips for the columns 'member_casual', 'rideable_type', 'trip_duration': \\n\\n{firstTen_trips}\\n\")  \n",
    "\n",
    "October_trips = three_Columns.loc['2024-10-15':'2024-10-17']  \n",
    "print(f\"Trips made in the date range '2024-10-15' to '2024-10-17': \\n\\n{October_trips}\\n\")  \n",
    "#print(October_trips)  \n",
    "\n",
    "Num_October_trips = len(October_trips)\n",
    "print(f\"Number of trips made during the date range '2024-10-15' to '2024-10-17': {Num_October_trips}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Reflect on this problem and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. `isin()` advantages: Compare using `isin(['A', 'B', 'C'])` versus `(col == 'A') | (col == 'B') | (col == 'C')`. Beyond readability, what practical advantage does `isin()` provide when filtering for many values (e.g., 20+ stations)?\n",
    "2. Conditional assignment order: In Task 3b, why did we initialize all values to \"Midday\" before assigning rush hour categories? What would go wrong if you assigned categories in a different order, or didn't set a default?\n",
    "3. `query()` vs boolean indexing: The `query()` method in Task 3c could have been written with boolean indexing instead. When would you choose `query()` over boolean indexing? When might boolean indexing be preferable despite being more verbose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### Problem 3 interpretation\n",
    "\n",
    "1. The syntax of `isin(['A', 'B', 'C'])` is simpler and does not require comparison and logical operators, including `==` and `|` in this case, which can reduce the error in code syntax.\n",
    "2. We initialized all the values to \"Midday\" for one specific reason and another reason that I believe could be true.\n",
    "   - The specific reason is that the `Morning Rush` and `Evening Rush` hours were certain values, and `Midday` hours were the rest of the values  \n",
    "   - Additionally, it ensures that none of the values would be labeled as a non-number, `NaN`, by default.\n",
    "   - If `Morning Rush` and `Evening Rush` categories were assigned first, the rest of the values, I believe, would be labeled as a non-number, `NaN`, by default\n",
    "3. I would choose to use `query()` over indexing when I need to find specific categories or ranges in several different columns.\n",
    "    - The one detail that can be forgettable, however, is including the `@` symbol when using a string, because this tells Python to look for a variable. In this case, it could be preferable to use Boolean indexing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)\n",
    "\n",
    "Pandas supports a variety of indexing paradigms, including bracket notation (`df['col']`), label-based indexing (`loc[]`), and position-based indexing (`iloc[]`). The lecture recommended using bracket notation only for columns, and loc/iloc for everything else. Explain the rationale: why is this approach better than using bracket notation for everything, even though `df[0:5]` technically works for row slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Graduate follow-up interpretation \n",
    "- `loc[]` provides a consistent way to access labeled rows and/or columns. With this method/approach, indexing can be accessed with the name of the columns and rows, versus having to use the column and row numbers.\n",
    "- `iloc[]` provides a consistent way to access data by the numbered index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Problem 4: Temporal Analysis and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Time-based patterns are crucial for understanding bike share usage. In this problem, you'll analyze when trips occur, how usage differs between rider types, and export filtered results. You'll use the datetime index you set in Problem 2 and the derived columns from Problems 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Task 4a: Identify Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Use the datetime index to extract temporal components and identify usage patterns:\n",
    "\n",
    "1. Extract the *hour* from the index and use `value_counts()` to find the most popular hour for trips. Report the peak hour and how many trips occurred during that hour.\n",
    "2. Extract the *day name* from the index and use `value_counts()` to find the busiest day of the week. Report the day and number of trips.\n",
    "3. Sort the results from step 2 to show days in order from Monday to Sunday (not by trip count). Use `sort_index()`.\n",
    "\n",
    "Hint: Use `.dt.hour` and `.dt.day_name()` on the datetime index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular hours for trips: \n",
      "started_at\n",
      "17    10574\n",
      "16     9705\n",
      "18     7939\n",
      "15     7541\n",
      "14     6392\n",
      "8      6391\n",
      "13     6131\n",
      "12     5818\n",
      "11     5070\n",
      "19     4885\n",
      "7      4776\n",
      "9      4703\n",
      "10     4498\n",
      "20     3461\n",
      "21     2725\n",
      "6      2281\n",
      "22     2107\n",
      "23     1507\n",
      "0      1121\n",
      "5       749\n",
      "1       719\n",
      "2       429\n",
      "3       252\n",
      "4       226\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Peak hour: 17\n",
      "\n",
      "Number of trips during the peak hour: 10574\n",
      "\n",
      "Number of trips during each of the week: \n",
      "started_at\n",
      "Monday       11531\n",
      "Tuesday      14970\n",
      "Wednesday    16513\n",
      "Thursday     16080\n",
      "Friday       13691\n",
      "Saturday     14427\n",
      "Sunday       12788\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Busiest day of the week: Wednesday\n",
      "\n",
      "Number of trips on the busiest day: 16513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4a code here...\n",
    "\n",
    "hours = rides_DataFrame_date.index.hour # Extracting hour values from the index\n",
    "hour_count = hours.value_counts()\n",
    "print(f\"Most popular hours for trips: \\n{hour_count}\\n\")\n",
    "\n",
    "peak_hour = hour_count.idxmax()\n",
    "print(f\"Peak hour: {peak_hour}\\n\")\n",
    "\n",
    "peak_count = hour_count.max()\n",
    "print(f\"Number of trips during the peak hour: {peak_count}\\n\")\n",
    "\n",
    "days = rides_DataFrame_date.index.day_name() \n",
    "#print(days)\n",
    "days_count = days.value_counts()\n",
    "\n",
    "days_ordered = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "days_count_ordered = days_count.reindex(days_ordered)\n",
    "print(f\"Number of trips during each of the week: \\n{days_count_ordered}\\n\")\n",
    "\n",
    "busiest_day = days_count.idxmax()\n",
    "print(f\"Busiest day of the week: {busiest_day}\\n\")\n",
    "\n",
    "numTrips_busiestDay = days_count.max()\n",
    "print(f\"Number of trips on the busiest day: {numTrips_busiestDay}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Task 4b: Compare Groups with groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Use `groupby()` (introduced in 07a) to compare trip characteristics across different groups:\n",
    "\n",
    "1. Calculate the average trip duration by rider type (`member_casual`). Which group takes longer trips on average?\n",
    "2. Calculate the average trip duration by bike type (`rideable_type`). Which bike type has the longest average trip?\n",
    "3. Count the number of trips by rider type using `groupby()` with `.size()`. Compare this with using `value_counts()` on the `member_casual` column - do they give the same result?\n",
    "\n",
    "Note: Use single-key groupby only (one column at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8d98e-a7dd-4b66-97d3-190eabe8ddf7",
   "metadata": {},
   "source": [
    "3. Result:\n",
    "    - `groupby()` with `.size()` and `value_counts('member_casual')` return a similar result.\n",
    "    - The only *difference* is the order of the categorys "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_casual\n",
      "casual    23.978046\n",
      "member    11.984493\n",
      "Name: trip_duration, dtype: float64\n",
      "\n",
      "Casual groups take longer trips on average.\n",
      "\n",
      "rideable_type\n",
      "classic_bike     20.337410\n",
      "electric_bike    12.033618\n",
      "Name: trip_duration, dtype: float64\n",
      "\n",
      "Classic_bikes take longer trips on average.\n",
      "\n",
      "Number of trips by each rider type: \n",
      "member_casual\n",
      "casual    34686\n",
      "member    65314\n",
      "Name: trip_duration, dtype: int64 \n",
      "\n",
      "Number of trips by each rider type: \n",
      "member_casual\n",
      "member    65314\n",
      "casual    34686\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzt0065\\AppData\\Local\\Temp\\ipykernel_3872\\829239588.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  rider_type = rides_DataFrame_date.groupby('member_casual')['trip_duration'].mean()\n",
      "C:\\Users\\dzt0065\\AppData\\Local\\Temp\\ipykernel_3872\\829239588.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bike_type = rides_DataFrame.groupby('rideable_type')['trip_duration'].mean()\n",
      "C:\\Users\\dzt0065\\AppData\\Local\\Temp\\ipykernel_3872\\829239588.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  rider_type = rides_DataFrame.groupby('member_casual')['trip_duration'].size()\n"
     ]
    }
   ],
   "source": [
    "# Task 4b code here...\n",
    "\n",
    "rider_type = rides_DataFrame_date.groupby('member_casual')['trip_duration'].mean()  \n",
    "print(rider_type)\n",
    "print(f\"\\n{rider_type.idxmax().capitalize()} groups take longer trips on average.\\n\")\n",
    "\n",
    "bike_type = rides_DataFrame.groupby('rideable_type')['trip_duration'].mean()  \n",
    "print(bike_type)\n",
    "print(f\"\\n{bike_type.idxmax().capitalize()}s take longer trips on average.\\n\")\n",
    "\n",
    "rider_type = rides_DataFrame.groupby('member_casual')['trip_duration'].size()  \n",
    "#print(rider_type)\n",
    "print(f\"Number of trips by each rider type: \\n{rider_type} \\n\")\n",
    "\n",
    "rider_type = rides_DataFrame.value_counts('member_casual')  \n",
    "#print(rider_type)\n",
    "print(f\"Number of trips by each rider type: \\n{rider_type} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Task 4c: Filter, Sample, and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Create a filtered dataset for weekend electric bike trips and export it:\n",
    "\n",
    "The provided code once again uses Path to create an `output` directory and constructs the full file path as `output/weekend_electric_trips.csv`. Use the `output_file` variable when calling `.to_csv()`.\n",
    "\n",
    "1. Filter for trips where `is_weekend == True` and `rideable_type == 'electric_bike'`\n",
    "2. Use `iloc[]` to select the first 1000 trips from this filtered dataset\n",
    "3. Use `reset_index()` to convert the datetime index back to a column (so it's included in the export)\n",
    "4. Export to CSV with filename `weekend_electric_trips.csv`, including only these columns: `started_at`, `ended_at`, `member_casual`, `trip_duration_min`, `time_of_day`\n",
    "5. Use `index=False` to avoid writing the default numeric index to the file\n",
    "\n",
    "After exporting, report how many total weekend electric bike trips existed before sampling to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started_at,ended_at,member_casual,trip_duration,time_of_day\n",
      "2024-10-01 00:05:44.954,2024-10-01 00:05:45.760,member,0.013433333333333334,Midday\n",
      "2024-10-01 00:06:12.035,2024-10-01 00:16:40.411,member,10.472933333333334,Midday\n",
      "2024-10-01 00:10:29.526,2024-10-01 00:17:04.344,casual,6.580299999999999,Midday\n",
      "2024-10-01 00:13:10.580,2024-10-01 00:13:26.944,casual,0.2727333333333333,Midday\n",
      "2024-10-01 00:13:18.415,2024-10-01 00:16:57.230,member,3.646916666666667,Midday\n",
      "2024-10-01 00:14:36.529,2024-10-01 00:23:45.959,casual,9.157166666666665,Midday\n",
      "2024-10-01 00:14:48.058,2024-10-01 00:15:21.393,member,0.5555833333333333,Midday\n",
      "2024-10-01 00:17:03.628,2024-10-01 00:17:18.589,member,0.24935000000000002,Midday\n",
      "2024-10-01 00:23:29.903,2024-10-01 00:24:08.807,casual,0.6484000000000001,Midday\n",
      "2024-10-01 00:24:43.011,2024-10-01 01:21:26.197,casual,56.71976666666667,Midday\n",
      "2024-10-01 00:24:51.949,2024-10-01 00:25:51.820,casual,0.99785,Midday\n",
      "2024-10-01 00:27:03.582,2024-10-01 00:33:10.798,member,6.1202 ...\n"
     ]
    }
   ],
   "source": [
    "# do not modify this setup code\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / 'weekend_electric_trips.csv'\n",
    "\n",
    "# Task 4c code here...\n",
    "# use the variable `output_file` as the filename for step 4\n",
    "\n",
    "weekend_electric_trips = rides_DataFrame_date.query('is_weekend == True and rideable_type == \"electric_bike\"')\n",
    "#print(weekend_electric_trips)\n",
    "output_file = weekend_electric_trips.iloc[:1000].reset_index()\n",
    "\n",
    "# Saving specific columns\n",
    "output_file.to_csv('weekend_electric_trips.csv', \n",
    "          columns=['started_at', 'ended_at', 'member_casual', 'trip_duration', 'time_of_day'],\n",
    "          index=False)\n",
    "\n",
    "# Verifying what was written to the csv file\n",
    "with open('weekend_electric_trips.csv', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content[:1000], '...')  # First 1000 characters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Reflect on this problem and answer the following questions:\n",
    "\n",
    "1. `groupby() conceptual model`: Explain in your own words what `groupby()` does. Use the phrase \"split-apply-combine\" in your explanation and describe what happens at each stage.\n",
    "2. `value_counts()` vs `groupby()`: In Task 4b.3, you compared two approaches for counting trips by rider type. When would you use `value_counts()` versus `groupby().size()`? Is there a situation where only one of them would work?\n",
    "3. Index management for export: In Task 4c, why did we use `reset_index()` before exporting? What would happen if you exported with the datetime index still in place and used `index=False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "#### Problem 4 interpretation here \n",
    "1. `groupby()` allows you to see a certain type of numeric data for each category in a column  \n",
    "    - The type of numeric data in this case is the duration of trips  \n",
    "    - This model uses the concept of \"split-apply-combine\": this explanation is below\n",
    "       - It splits/takes numeric values from another column and displays them with another column \n",
    "       - `bike_type = rides_DataFrame.groupby('rideable_type')['trip_duration'].mean() `\n",
    "       - In the code above, `groupby()` *splits* the columns `rideable_type` and `trip_duration` from `rides_DataFrame` and then combines them to be side-by-side columns \n",
    "          - **Additionally**, it applies and displays the average/mean duration for each category of `rideable_type`\n",
    "1. From the result of using these two methods, it appears that `groupby().size()` returns the categories in alphabetical order.\n",
    "    - Therefore, I guess I would use `value_counts()` when I am not worried about the order of the categories\n",
    "    - I cannot think of a situation where only one of them would work. \n",
    "   \n",
    "1. We reset the index so that `start_at` could be accessed as a column again, instead of the index.\n",
    "   - If exported with the datetime index in place in addition to using `index=False`, an error will be returned \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Compare `CSV` and _pickle_ formats for data storage and retrieval.\n",
    "\n",
    "Pickle is Python's built-in serialization format that saves Python objects exactly as they exist in memory, preserving all data types, structures, and metadata. Unlike CSV (which converts everything to text), pickle is binary (not human readable) and maintains the complete state of your DataFrame. Also, pickle files only work in Python, while CSV is universal. Read more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html).\n",
    "\n",
    "The code below investigates an interesting pattern: Do riders take longer trips from scenic lakefront stations even during rush hours? This could indicate tourists or recreational riders using these popular locations for leisure trips during typical commute times. The analysis filters for trips over 15 minutes that started from lakefront stations during morning (7-9am) or evening (4-6pm) rush hours, sorted by duration to see the longest trips first.\n",
    "\n",
    "Run the code below, then answer the interpretation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 long rush-hour trips from lakefront stations\n",
      "\n",
      "CSV file size: 40.57 KB\n",
      "Pickle file size: 55.16 KB\n",
      "Size difference: 14.59 KB\n",
      "\n",
      "Load time comparison:\n",
      "CSV:\n",
      "2.75 ms ± 142 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Pickle:\n",
      "1.18 ms ± 80.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Data types from CSV (without parse_dates):\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "end_station_name       object\n",
      "member_casual          object\n",
      "rideable_type          object\n",
      "trip_duration         float64\n",
      "dtype: object\n",
      "\n",
      "Data types from Pickle:\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name          category\n",
      "end_station_name            category\n",
      "member_casual               category\n",
      "rideable_type               category\n",
      "trip_duration                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# the following lines were commented out since they were run in 4c\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('output')\n",
    "\n",
    "rides = rides_DataFrame_date\n",
    "\n",
    "csv_file = output_dir / 'lakefront_rush_trips.csv'\n",
    "pickle_file = output_dir / 'lakefront_rush_trips.pkl'\n",
    "\n",
    "# Filter for interesting pattern: Long trips (>15 min) during rush hours \n",
    "# from lakefront stations, sorted by duration\n",
    "lakefront_rush = (rides\n",
    "    .loc[(rides.index.hour.isin([7, 8, 9, 16, 17, 18]))]\n",
    "    .loc[(rides['start_station_name'].str.contains('Lake Shore|Lakefront', \n",
    "                                                    case=False, \n",
    "                                                    na=False))]\n",
    "    .loc[rides['trip_duration'] > 15]\n",
    "    .sort_values('trip_duration', ascending=False)\n",
    "    .head(1000)\n",
    "    .reset_index()\n",
    "    [['started_at', 'ended_at', 'start_station_name', 'end_station_name',\n",
    "      'member_casual', 'rideable_type', 'trip_duration']]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(lakefront_rush)} long rush-hour trips from lakefront stations\")\n",
    "\n",
    "# Export to both formats\n",
    "lakefront_rush.to_csv(csv_file, index=False)\n",
    "lakefront_rush.to_pickle(pickle_file)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / 1024  # Convert to KB\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024\n",
    "print(f\"\\nCSV file size: {csv_size:.2f} KB\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(csv_size - pickle_size):.2f} KB\")\n",
    "\n",
    "# Compare load times\n",
    "print(\"\\nLoad time comparison:\")\n",
    "print(\"CSV:\")\n",
    "%timeit pd.read_csv(csv_file)\n",
    "print(\"\\nPickle:\")\n",
    "%timeit pd.read_pickle(pickle_file)\n",
    "\n",
    "# Check data type preservation\n",
    "# Note: CSV load without parse_dates loses datetime types\n",
    "csv_loaded = pd.read_csv(csv_file)\n",
    "pickle_loaded = pd.read_pickle(pickle_file)\n",
    "\n",
    "print(\"\\nData types from CSV (without parse_dates):\")\n",
    "print(csv_loaded.dtypes)\n",
    "print(\"\\nData types from Pickle:\")\n",
    "print(pickle_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "After running the code, answer these questions:\n",
    "\n",
    "1. Method chaining: The analysis uses method chaining with a specific formatting pattern:\n",
    "\n",
    "   ```python\n",
    "   result = (df\n",
    "       .method1()\n",
    "       .method2()\n",
    "       .method3()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This wraps the entire chain in parentheses, allowing each method to appear on its own line without backslashes. Discuss why this makes formatting more readable, how it makes debugging easier, how it relates to seeing changes in the code with git diff, and what downsides heavy chaining might have.\n",
    "3. Data types: Compare the dtypes from CSV versus pickle. What types were preserved by pickle that were lost in CSV? Why is this preservation significant for subsequent analysis?\n",
    "4. Trade-offs: Given your observations about size, speed, and type preservation, when would you choose pickle over CSV for your work? When would CSV still be the better choice despite pickle's advantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "#### Graduate follow-up interpretation here\n",
    "1. From a coding standpoint, a single file of code could be hundreds of lines long. Therefore, formatting code to be more readable, like method chaining, can help the developer/creator of the code be able to understand and read their code more efficiently because it is in a neat format.\n",
    "    - Since each method is on a *separate line*, debugging code can be less stressful\n",
    "    - The downsides of heavy chaining could lead to the overuse of method chaining, such as an unnecessary amount of coding lines\n",
    "1. The dtypes `datetime64[ns]` for the columns `started_at` and `ended_at` were preserved by pickle.\n",
    "   - This preservation is significant because it labels this variable by the dtype that it is known for in the data set, which is a variable representing a date. A subsequent analysis might call for the date to be labeled by its specific data type.\n",
    "1. I would use pickle if I had needed access to specific dtypes in a high-pressure environment where speed and efficiency were necessary, and of course, I had computer memory that could handle the file size for pickle.\n",
    "    - However, CSV would be the better choice if I did not have the memory availability to handle a large file size.\n",
    "          - In this case, the runtime would be a little slower CSV would still help me see whether a value is a number, `int`, or `float`, or an object, a string. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Address the following questions in a markdown cell:\n",
    "\n",
    "1. NumPy vs Pandas\n",
    "   - What was the biggest conceptual shift moving from NumPy arrays to Pandas DataFrames?\n",
    "   - Which Pandas concept was most challenging: indexing (loc/iloc), missing data, datetime operations, or method chaining? How did you work through it?\n",
    "2. Real Data Experience\n",
    "   - How did working with real CSV data (with missing values, datetime strings, etc.) differ from hw2b's synthetic NumPy arrays?\n",
    "   - Based on this assignment, what makes Pandas well-suited for data analysis compared to pure NumPy?\n",
    "3. Learning & Application\n",
    "   - Which new skill from this assignment will be most useful for your own data work?\n",
    "   - On a scale of 1-10, how prepared do you feel to use Pandas for your own projects? What would increase that score?\n",
    "4. Feedback\n",
    "   - Time spent: ___ hours (breakdown optional)\n",
    "   - Most helpful part of the assignment: ___\n",
    "   - One specific improvement suggestion: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "1. - The biggest conceptual was prabably understanding how to use the index feature of Pandas DataFrames\n",
    "   - probably learning how to use loc/iloc for indexing properly, and understanding why I was getting True and False to show up using `isin()`\n",
    "   - I used Claude to troubleshoot and help me understand output errors, and I also met with the TA to clarify some concepts and troubleshoot code\n",
    "2. - Working with real CSV data allowed me to see the format of real data. It was interesting to see how missing values were displayed as NaN and how many missing values there were.\n",
    "   - Pandas includes many methods and functions that allow manipulation and sorting of data a more efficient process versus pure NumPy\n",
    "3. Many new skills would be most useful, such as using `loc()` / `iloc()` and `groupby()`.\n",
    "     - These are powerful tools when needing to find ranges of data and/or separate and combine data\n",
    "   - I feel pretty prepared, being that I have never used a language this powerful\n",
    "     - Definitely just having time to practice all the methods and techniques used in Pandas\n",
    "4. - probably around 15 hours, since I am very new to these powerful languages\n",
    "   - The steps, including what functions and methods to use initially\n",
    "   - Definitely the workload of the assignment. It can be a lot to write a good interpretation in addition to understanding and writing code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b11ec7-97ac-4459-8bf0-472454840eb7",
   "metadata": {},
   "source": [
    "This URL is to my claude chat where I asked for clarifications:\n",
    "\n",
    "https://claude.ai/share/ea251f8d-4739-46c2-ac57-1441ea5c6ded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc09e9-9104-4cd7-8b07-f63a78f6c0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
